{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adopted and modified from [AfriSenti Semeval](https://github.com/afrisenti-semeval/afrisent-semeval-2023). Originally created *by* [Idris Abdulmumin](https://www.hausanlp.org/author/idris-abdulmuminu/), [David Adelani](https://dadelani.github.io/) and [Shamsuddeen Hassan Muhammad](https://www.hausanlp.org/author/shamsuddeen-hassan-muhammad/).\n",
        "\n"
      ],
      "metadata": {
        "id": "pWEuSvUzNHrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWapyu67M7kF",
        "outputId": "6b70aa57-63d2-45f8-ae08-a05239cc9fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Please don not edit anything here\n",
        "languages = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo', 'twi', 'kr', 'ts']"
      ],
      "metadata": {
        "id": "3VFS3wGRN8d9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "import os\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/afrisent-semeval-2023'\n",
        "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
        "\n",
        "if not os.path.isdir(PROJECT_DIR):\n",
        "  !git clone {PROJECT_GITHUB_URL}\n",
        "else:\n",
        "  %cd {PROJECT_DIR}\n",
        "  !git pull {PROJECT_GITHUB_URL}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX1ajsOhN-Wv",
        "outputId": "dd07a959-1a4a-4597-fa46-13f538563689"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/afrisent-semeval-2023\n",
            "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/afrisent-semeval-2023/starter_kit/requirements.txt"
      ],
      "metadata": {
        "id": "jrlWd62UOAUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data Paths\n",
        "\n",
        "TASK = 'SubtaskA'\n",
        "if TASK == 'SubtaskB': \n",
        "  TRAINING_DATA_DIR = os.path.join(PROJECT_DIR, TASK)\n",
        "else:\n",
        "  TRAINING_DATA_DIR = os.path.join(PROJECT_DIR, TASK, 'train')\n",
        "FORMATTED_TRAIN_DATA = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data')\n",
        "\n",
        "if os.path.isdir(TRAINING_DATA_DIR):\n",
        "  print('Data directory found.')\n",
        "  if not os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "    print('Creating directory to store formatted data.')\n",
        "    os.mkdir(FORMATTED_TRAIN_DATA)\n",
        "else:\n",
        "  print(TRAINING_DATA_DIR + ' is not a valid directory or does not exist!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8wH_IbQOCqN",
        "outputId": "af9a9d10-3015-4025-8482-62e8e2b758e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {TRAINING_DATA_DIR}\n",
        "\n",
        "training_files = os.listdir()\n",
        "\n",
        "if len(training_files) > 0:\n",
        "  for training_file in training_files:\n",
        "    if training_file.endswith('.tsv'):\n",
        "\n",
        "      data = training_file.split('_')[0]\n",
        "      if not os.path.isdir(os.path.join(FORMATTED_TRAIN_DATA, data)):\n",
        "        print(data, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(FORMATTED_TRAIN_DATA, data))\n",
        "      \n",
        "      df = pd.read_csv(training_file, sep='\\t', names=['ID', 'text', 'label'], header=0)\n",
        "      df[['text', 'label']].sample(frac=1, random_state=42).to_csv(os.path.join(FORMATTED_TRAIN_DATA, data, 'train.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' skipped!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ],
      "metadata": {
        "id": "tU9AcCZbOJdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "  print('Data directory found.')\n",
        "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev')\n",
        "  if not os.path.isdir(SPLITTED_DATA):\n",
        "    print('Creating directory to store train, dev and test splits.')\n",
        "    os.mkdir(SPLITTED_DATA)\n",
        "else:\n",
        "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {FORMATTED_TRAIN_DATA}\n",
        "formatted_training_files = os.listdir()\n",
        "\n",
        "if len(formatted_training_files) > 0:\n",
        "  for data_name in formatted_training_files:\n",
        "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
        "    if os.path.isfile(formatted_training_file):\n",
        "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
        "      train, dev = train_test_split(labeled_tweets, test_size=0.3, shuffle=True, random_state=42)\n",
        "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
        "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
        "\n",
        "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
        "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddV_-74NONtb",
        "outputId": "47587f04-2982-40fe-d794-b37025886b96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory found.\n",
            "/content/drive/MyDrive/afrisent-semeval-2023/SubtaskA/train/formatted-train-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {PROJECT_DIR}\n",
        "languages = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo', 'twi', 'kr', 'ts']\n",
        "# Language to train sentiment classifier for\n",
        "LANGUAGE_CODE = 'ha'\n",
        "\n",
        "#To avoid overwriting existing models, the model version should be edited\n",
        "MODEL_VERSION = '3'\n",
        "\n",
        "if LANGUAGE_CODE in languages:\n",
        "  # Model Training Parameters\n",
        "  MODEL_NAME_OR_PATH = 'Davlan/afro-xlmr-large'\n",
        "  BATCH_SIZE = 16\n",
        "  LEARNING_RATE = 1e-5\n",
        "  NUMBER_OF_TRAINING_EPOCHS = 5\n",
        "  WEIGHT_DECAY = 0.01\n",
        "  MAXIMUM_SEQUENCE_LENGTH = 128\n",
        "  SAVE_STEPS = -1\n",
        "  OVERWRITE_OUPTUT_DIR = True\n",
        "  LRS = 'linear'\n",
        "  print('Everything set. You can now start model training.')\n",
        "#(choose from 'adamw_hf', 'adamw_torch', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'adamw_anyprecision', 'sgd', 'adagrad')\n",
        "else:\n",
        "  print(\"Invalid language code/Dataset not released. Please input any of the following released data\\n\\n\\t- 'am'\\n\\t- 'dz'\\n\\t- 'ha'\\n\\t- 'ig'\\n\\t- 'ma'\\n\\t- 'pcm'\\n\\t- 'pt'\\n\\t- 'sw'\\n\\t- 'yo'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4fVMTwZOT75",
        "outputId": "2d210cdb-62a1-41c2-d5a8-974b0f80b6f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/afrisent-semeval-2023\n",
            "Everything set. You can now start model training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE, 'v_' + MODEL_VERSION)\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --overwrite_output_dir yes\\\n",
        "  --greater_is_better  no\\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --weight_decay {WEIGHT_DECAY} \\\n",
        "  --learning_rate {LEARNING_RATE} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS} \\\n",
        "  --lr_scheduler_type {LRS} \\\n",
        "  --optim {'adafactor'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWl2krxIOZqz",
        "outputId": "430fc77d-2927-489f-d3cf-c046b3f95d0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "WARNING:__main__:Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/afrisent-semeval-2023/models/ha/v_3/runs/Feb02_18-33-45_9627fe3ce96b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adafactor,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/afrisent-semeval-2023/models/ha/v_3,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/afrisent-semeval-2023/models/ha/v_3,\n",
            "save_on_each_node=False,\n",
            "save_steps=-1,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            "xpu_backend=None,\n",
            ")\n",
            "['neutral', 'positive', 'negative']\n",
            "Downloading (…)lve/main/config.json: 100% 714/714 [00:00<00:00, 80.3kB/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-02 18:33:46,125 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-02 18:33:46,127 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"Davlan/afro-xlmr-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 399/399 [00:00<00:00, 146kB/s]\n",
            "Downloading (…)ncepiece.bpe.model\";: 100% 5.07M/5.07M [00:00<00:00, 41.4MB/s]\n",
            "Downloading (…)\"tokenizer.json\";: 100% 17.1M/17.1M [00:00<00:00, 76.4MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 239/239 [00:00<00:00, 91.2kB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-02 18:33:47,835 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-02 18:33:47,835 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-02 18:33:47,835 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-02 18:33:47,835 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-02 18:33:47,835 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/tokenizer_config.json\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 1.12G/1.12G [00:17<00:00, 64.9MB/s]\n",
            "[INFO|modeling_utils.py:2275] 2023-02-02 18:34:06,709 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Davlan--afro-xlmr-large/snapshots/a997a617964aa699877cee60b2dfcce177469fb3/pytorch_model.bin\n",
            "tcmalloc: large alloc 1120288768 bytes == 0x1ef92000 @  0x7f2637915680 0x7f2637936824 0x7f2637936b8a 0x7f254f676dc5 0x7f254f654053 0x7f257ea9ef20 0x7f25a5702d12 0x7f25a52e9f00 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x5f32d1 0x5f35b5 0x485467 0x504b7b 0x56bbe1 0x569d8a 0x5f60c3 0x5f52b2 0x56d2bc 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x56bab6\n",
            "[WARNING|modeling_utils.py:2847] 2023-02-02 18:34:19,751 >> Some weights of the model checkpoint at Davlan/afro-xlmr-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2859] 2023-02-02 18:34:19,751 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset: 100% 10/10 [00:01<00:00,  8.19ba/s]\n",
            "INFO:__main__:Sample 1824 of the training set: {'text': '@user Lemme guess: domin yada farfaganda ko @user 🤔 https://t.co/NoFYnmTMWX', 'label': 0, '__index_level_0__': 1824, 'input_ids': [0, 1374, 65918, 636, 3019, 57767, 12, 15102, 117580, 2060, 1021, 38269, 298, 1374, 65918, 6, 243691, 3975, 696, 18, 5, 587, 64, 5021, 919, 1723, 19, 39, 10111, 1456, 1542, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 409 of the training set: {'text': '@user pic nan yamin kyaufa.❤️❤️ https://t.co/or5UnkDsr0', 'label': 1, '__index_level_0__': 409, 'input_ids': [0, 1374, 65918, 8834, 25990, 151, 1249, 152527, 1021, 5, 50966, 15755, 50966, 15755, 3975, 696, 18, 5, 587, 64, 748, 758, 14256, 92, 397, 7, 42, 2389, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 4506 of the training set: {'text': '@user Kinga Ni ko rahma nayi matukar bakin ciki😠 da Naga wanan hotun Nan kina jamana zagi a matsayin ki n musulma tun Muna damuwa har mun sallama ki Amma abin takaici shine abin ya tashi daka kanmu ta dalilin ki anyi batanci ga fiyayyen halitta Annabin Muhammad S A W 😭😰😥😢', 'label': 2, '__index_level_0__': 4506, 'input_ids': [0, 1374, 65918, 18813, 11, 1520, 298, 6, 3962, 192, 24, 2387, 49641, 1153, 156691, 94318, 247586, 48, 117436, 8971, 19, 8010, 309, 37226, 24222, 79, 4707, 80, 735, 10, 166766, 200, 653, 108947, 23076, 8230, 124162, 22539, 47154, 182, 4829, 154304, 11, 200, 16355, 75718, 10881, 2624, 158618, 75718, 151, 39562, 14, 48, 161, 203, 561, 308, 134415, 73, 200, 2499, 14, 8336, 7700, 914, 1238, 22678, 12222, 12967, 2310, 7714, 7568, 10571, 159, 62, 601, 6, 232773, 247441, 245285, 240565, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Running tokenizer on validation dataset: 100% 5/5 [00:00<00:00, 10.60ba/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 3.41MB/s]\n",
            "[INFO|trainer.py:710] 2023-02-02 18:34:21,932 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1650] 2023-02-02 18:34:21,948 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-02 18:34:21,948 >>   Num examples = 9920\n",
            "[INFO|trainer.py:1652] 2023-02-02 18:34:21,948 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1653] 2023-02-02 18:34:21,948 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1654] 2023-02-02 18:34:21,948 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1655] 2023-02-02 18:34:21,948 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1656] 2023-02-02 18:34:21,948 >>   Total optimization steps = 3100\n",
            "[INFO|trainer.py:1657] 2023-02-02 18:34:21,949 >>   Number of trainable parameters = 559893507\n",
            "  0% 0/3100 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"starter_kit/run_textclass.py\", line 452, in <module>\n",
            "    main()\n",
            "  File \"starter_kit/run_textclass.py\", line 404, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2557, in training_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  0% 0/3100 [01:23<?, ?it/s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {PROJECT_DIR}\n",
        "\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE, 'v_' + MODEL_VERSION)\n",
        "FILE_NAME = os.path.join(PROJECT_DIR, TASK, 'test', LANGUAGE_CODE + '_test_participants.tsv')\n",
        "TEXT_COLUMN = 'tweet'\n",
        "\n",
        "!python starter_kit/run_predict.py \\\n",
        "  --model_path {OUTPUT_DIR} \\\n",
        "  --file_name {FILE_NAME} \\\n",
        "  --text_column {TEXT_COLUMN} \\\n",
        "  --lang_code {LANGUAGE_CODE}"
      ],
      "metadata": {
        "id": "t4zJILoLOdFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}